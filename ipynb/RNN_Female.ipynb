{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data  \n",
    "Mark end of message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_table('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\f.txt', header=None ,error_bad_lines=False)\n",
    "df1=df1.apply(lambda x: x+' סוףהודעה')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concat all messages\n",
    "X2=df1[0].tolist()\n",
    "text= ' '.join(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a sentence into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 25577\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "words1=text_to_word_sequence(text, lower=False, split=\" \")\n",
    "words = sorted(text_to_word_sequence(text, lower=False, split=\" \"))\n",
    "words.append(' ')\n",
    "vocab_size = len(words)\n",
    "print('total words:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetUniqueWords(words):\n",
    "    words_set = set()\n",
    "    for word in words:                \n",
    "        words_set.add(word)\n",
    "    return words_set\n",
    "\n",
    "unique_words = GetUniqueWords(words)\n",
    "number_of_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build index_to_word and word_to_index vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "idx = [word_indices[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sen=df1[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut the text in semi-redundant sequences of maxlen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number sequences: 8524\n"
     ]
    }
   ],
   "source": [
    "maxlen = 5\n",
    "step = 3\n",
    "sentences = []\n",
    "next_words = []\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen words\n",
    "for i in range(0, len(words1) - maxlen, step):\n",
    "    sentences.append(words1[i: i + maxlen])\n",
    "    next_words.append(words1[i + maxlen])\n",
    "print('number sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp=pd.DataFrame(sentences)\n",
    "sentences=tmp[0]+' '+tmp[1]+' '+tmp[2]+' '+tmp[3]+' '+tmp[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(words)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(words)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(text_to_word_sequence(sentence, lower=False, split=\" \")):\n",
    "        X[i, t, word_indices[word]] = 1\n",
    "    y[i, word_indices[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_indices) + 1, 300))\n",
    "for word, i in word_indices.items():        \n",
    "    embedding_vector = sentences.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n",
    "Is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem\n",
    "\n",
    "* LSTM layer\n",
    "* A Dense layer with len(words) nodes\n",
    "* Activation function= softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               13161472  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25577)             3299433   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25577)             0         \n",
      "=================================================================\n",
      "Total params: 16,460,905\n",
      "Trainable params: 16,460,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(words))))\n",
    "#a Dense layer with len(words) nodes\n",
    "model.add(Dense(len(words)))\n",
    "#Activation function= softmax\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model</br>  \n",
    "We use the RMSProp optimizer</br>  \n",
    "We use the sparse_categorical_crossentropy loss that accepts sparse labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test, train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from array import array\n",
    "import random\n",
    "from random import randrange\n",
    "test_x=[]\n",
    "test_y=[]\n",
    "\n",
    "index=[]\n",
    "for i in range(0,900):\n",
    "    random_index = randrange(0,len(X))\n",
    "    index.append(random_index)\n",
    "    test_x.append(X[random_index])\n",
    "    test_y.append(y[random_index])\n",
    "    \n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "train_x = np.delete(X, index, axis=0)\n",
    "train_y = np.delete(y, index, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train the model, fit it to the data\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,validation_split=0.2)\n",
    "model.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 7s     \n",
      "Accuracy: 75.5555555556 %\n"
     ]
    }
   ],
   "source": [
    "scores = model1.evaluate(test_x, test_y, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model</BR>  \n",
    "Fit it to the all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train the model, fit it to the data\n",
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,validation_split=0.2)\n",
    "\n",
    "#save the model\n",
    "model.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8524/8524 [==============================] - 75s    \n",
      "Accuracy: 99.8005631159 %\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, y, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def concatenate_list_data(list):\n",
    "    result= ''\n",
    "    for element in list:\n",
    "        result += str(element)+\" \"\n",
    "    return result[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#generate new sentences\n",
    "to_file=[]\n",
    "for i in range(0,1500):\n",
    "    #select seed index- first word of the sentance\n",
    "    start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "    while(words1[start_index:start_index+1][0] == 'סוףהודעה'):\n",
    "        start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "\n",
    "    for diversity in [1.0]:\n",
    "        generated = ''\n",
    "        sentence = words1[start_index: start_index+1]\n",
    "        generated += concatenate_list_data(sentence)\n",
    "\n",
    "        sys.stdout.write(generated)\n",
    "        sen_to_file=generated\n",
    "\n",
    "        next_word=''\n",
    "        z=0\n",
    "        for i in range(0,12):\n",
    "            x = np.zeros((1, maxlen, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t, word_indices[word]] = 1.\n",
    "            #predict the next words\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            generated += next_word+' '\n",
    "            sentence = sentence[1:] + list([next_word])\n",
    "            if(next_word == 'סוףהודעה'):\n",
    "                break\n",
    "            sen_to_file+=' '+next_word\n",
    "            z+=1\n",
    "            sys.stdout.write(' '+next_word)              \n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        if(len(sen_to_file.split(' '))>1):\n",
    "            to_file.append(sen_to_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save to file\n",
    "with open('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\ff_new.txt', 'w', newline='\\n', encoding='utf-8') as txt_file:\n",
    "    writer = csv.writer(txt_file, delimiter='\\n')\n",
    "    writer.writerow(to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#read the data\n",
    "df1 = pd.read_table('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\f.txt', header=None ,error_bad_lines=False)\n",
    "#mark end of message\n",
    "df1=df1.apply(lambda x: x+' סוףהודעה')\n",
    "\n",
    "# import re\n",
    "regex = re.compile('[^םןאבגדהוזחטיכלמנסעפצקרשתץףך?! ].*')\n",
    "df1[0]=df1[0].apply(lambda x: regex.sub('', x))\n",
    "df1=df1.dropna()\n",
    "df1=df1[df1[0]!=' ']\n",
    "\n",
    "#concat all messages\n",
    "text= ' '.join(df1[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 19494\n",
      "unique_words: 4629\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "words1=text_to_word_sequence(text, lower=False, split=\" \")\n",
    "words = sorted(text_to_word_sequence(text, lower=False, split=\" \"))\n",
    "words.append(' ')\n",
    "vocab_size = len(words)\n",
    "print('total words:', vocab_size)\n",
    "\n",
    "unique_words = GetUniqueWords(words)\n",
    "number_of_words = len(unique_words)\n",
    "print('unique_words:', number_of_words)\n",
    "\n",
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(unique_words))\n",
    "idx = [word_indices[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               13161472  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25577)             3299433   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25577)             0         \n",
      "=================================================================\n",
      "Total params: 16,460,905\n",
      "Trainable params: 16,460,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "cs = 2\n",
    "\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, 1)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, 1)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, 1)] \n",
    "\n",
    "x1 = np.array(c1_dat)\n",
    "x2 = np.array(c2_dat)\n",
    "x3 = np.array(c3_dat)\n",
    "    \n",
    "input_ = np.stack([x1,x2],axis=1)\n",
    "output_ = np.stack([x3],axis=1)\n",
    "\n",
    "n_fac = 42\n",
    "n_hidden = 256\n",
    "\n",
    "#build the model \n",
    "model3=Sequential([\n",
    "    Embedding(number_of_words, n_fac, input_length=cs),\n",
    "    LSTM(n_hidden, return_sequences=False, activation='relu'),        \n",
    "    Dense(number_of_words, activation='softmax'),\n",
    "])    \n",
    "  \n",
    "print(model.summary()) \n",
    "#compile\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop' ,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from array import array\n",
    "import random\n",
    "from random import randrange\n",
    "#split to train and test set\n",
    "\n",
    "test_x=[]\n",
    "test_y=[]\n",
    "\n",
    "index=[]\n",
    "for i in range(0,900):\n",
    "    random_index = randrange(0,len(input_))\n",
    "    index.append(random_index)\n",
    "    test_x.append(input_[random_index])\n",
    "    test_y.append(output_[random_index])\n",
    "    \n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "train_x = np.delete(input_, index, axis=0)\n",
    "train_y = np.delete(output_, index, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.9599 - acc: 0.6384    \n",
      "Epoch 2/30\n",
      "18610/18610 [==============================] - 28s - loss: 2.8953 - acc: 0.6481    \n",
      "Epoch 3/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.8324 - acc: 0.6569    \n",
      "Epoch 4/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.7728 - acc: 0.6642    \n",
      "Epoch 5/30\n",
      "18610/18610 [==============================] - 28s - loss: 2.7131 - acc: 0.6715    \n",
      "Epoch 6/30\n",
      "18610/18610 [==============================] - 29s - loss: 2.6540 - acc: 0.6794    \n",
      "Epoch 7/30\n",
      "18610/18610 [==============================] - 28s - loss: 2.5974 - acc: 0.6854    \n",
      "Epoch 8/30\n",
      "18610/18610 [==============================] - 26s - loss: 2.5405 - acc: 0.6913    \n",
      "Epoch 9/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.4839 - acc: 0.6983    \n",
      "Epoch 10/30\n",
      "18610/18610 [==============================] - 26s - loss: 2.4279 - acc: 0.7041    \n",
      "Epoch 11/30\n",
      "18610/18610 [==============================] - 26s - loss: 2.3750 - acc: 0.7113    \n",
      "Epoch 12/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.3231 - acc: 0.7179    \n",
      "Epoch 13/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.2706 - acc: 0.7237    \n",
      "Epoch 14/30\n",
      "18610/18610 [==============================] - 27s - loss: 2.2204 - acc: 0.7304    \n",
      "Epoch 15/30\n",
      "18610/18610 [==============================] - 28s - loss: 2.1713 - acc: 0.7358    \n",
      "Epoch 16/30\n",
      "18610/18610 [==============================] - 28s - loss: 2.1225 - acc: 0.7414    \n",
      "Epoch 17/30\n",
      "18610/18610 [==============================] - 29s - loss: 2.0724 - acc: 0.7472    \n",
      "Epoch 18/30\n",
      "18610/18610 [==============================] - 28s - loss: 2.0215 - acc: 0.7538    \n",
      "Epoch 19/30\n",
      "18610/18610 [==============================] - 27s - loss: 1.9710 - acc: 0.7601    \n",
      "Epoch 20/30\n",
      "18610/18610 [==============================] - 27s - loss: 1.9197 - acc: 0.7691    \n",
      "Epoch 21/30\n",
      "18610/18610 [==============================] - 28s - loss: 1.8699 - acc: 0.7749    \n",
      "Epoch 22/30\n",
      "18610/18610 [==============================] - 27s - loss: 1.8177 - acc: 0.7816    \n",
      "Epoch 23/30\n",
      "18610/18610 [==============================] - 26s - loss: 1.7671 - acc: 0.7864    \n",
      "Epoch 24/30\n",
      "18610/18610 [==============================] - 26s - loss: 1.7108 - acc: 0.7908    \n",
      "Epoch 25/30\n",
      "18610/18610 [==============================] - 26s - loss: 1.6529 - acc: 0.7945    \n",
      "Epoch 26/30\n",
      "18610/18610 [==============================] - 27s - loss: 1.6027 - acc: 0.7977    \n",
      "Epoch 27/30\n",
      "18610/18610 [==============================] - 27s - loss: 1.5283 - acc: 0.7999    \n",
      "Epoch 28/30\n",
      "18610/18610 [==============================] - 29s - loss: 1.4646 - acc: 0.8033    \n",
      "Epoch 29/30\n",
      "18610/18610 [==============================] - 29s - loss: 1.4087 - acc: 0.8066    \n",
      "Epoch 30/30\n",
      "18610/18610 [==============================] - 27s - loss: 1.3559 - acc: 0.8140    \n"
     ]
    }
   ],
   "source": [
    "model3.fit(train_x, y=train_y, batch_size=120, epochs=30, verbose=1)\n",
    "model3.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F3_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F3_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832/900 [==========================>...] - ETA: 0sAccuracy cv: 70.1111111111 %\n"
     ]
    }
   ],
   "source": [
    "scores = model4.evaluate(test_x, test_y, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model</BR>  \n",
    "Fit it to the all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "19491/19491 [==============================] - 31s - loss: 7.0879 - acc: 0.3479    \n",
      "Epoch 2/30\n",
      "19491/19491 [==============================] - 32s - loss: 4.6981 - acc: 0.4768    \n",
      "Epoch 3/30\n",
      "19491/19491 [==============================] - 31s - loss: 3.9711 - acc: 0.5406    \n",
      "Epoch 4/30\n",
      "19491/19491 [==============================] - 32s - loss: 3.5823 - acc: 0.5849    \n",
      "Epoch 5/30\n",
      "19491/19491 [==============================] - 32s - loss: 3.2815 - acc: 0.6183    \n",
      "Epoch 6/30\n",
      "19491/19491 [==============================] - 33s - loss: 3.0589 - acc: 0.6415    \n",
      "Epoch 7/30\n",
      "19491/19491 [==============================] - 29s - loss: 2.8893 - acc: 0.6572    \n",
      "Epoch 8/30\n",
      "19491/19491 [==============================] - 29s - loss: 2.7390 - acc: 0.6713    \n",
      "Epoch 9/30\n",
      "19491/19491 [==============================] - 29s - loss: 2.6015 - acc: 0.6854    \n",
      "Epoch 10/30\n",
      "19491/19491 [==============================] - 30s - loss: 2.4740 - acc: 0.6972    \n",
      "Epoch 11/30\n",
      "19491/19491 [==============================] - 31s - loss: 2.3539 - acc: 0.7080    \n",
      "Epoch 12/30\n",
      "19491/19491 [==============================] - 35s - loss: 2.2400 - acc: 0.7201    \n",
      "Epoch 13/30\n",
      "19491/19491 [==============================] - 30s - loss: 2.1254 - acc: 0.7295    \n",
      "Epoch 14/30\n",
      "19491/19491 [==============================] - 29s - loss: 2.0170 - acc: 0.7408    \n",
      "Epoch 15/30\n",
      "19491/19491 [==============================] - 29s - loss: 1.9098 - acc: 0.7522    \n",
      "Epoch 16/30\n",
      "19491/19491 [==============================] - 32s - loss: 1.8064 - acc: 0.7639    \n",
      "Epoch 17/30\n",
      "19491/19491 [==============================] - 29s - loss: 1.7105 - acc: 0.7763    \n",
      "Epoch 18/30\n",
      "19491/19491 [==============================] - 30s - loss: 1.6188 - acc: 0.7904    \n",
      "Epoch 19/30\n",
      "19491/19491 [==============================] - 29s - loss: 1.5356 - acc: 0.8023    \n",
      "Epoch 20/30\n",
      "19491/19491 [==============================] - 30s - loss: 1.4592 - acc: 0.8129    \n",
      "Epoch 21/30\n",
      "19491/19491 [==============================] - 31s - loss: 1.3898 - acc: 0.8235    \n",
      "Epoch 22/30\n",
      "19491/19491 [==============================] - 32s - loss: 1.3235 - acc: 0.8317    \n",
      "Epoch 23/30\n",
      "19491/19491 [==============================] - 33s - loss: 1.2675 - acc: 0.8416    \n",
      "Epoch 24/30\n",
      "19491/19491 [==============================] - 31s - loss: 1.2050 - acc: 0.8519    \n",
      "Epoch 25/30\n",
      "19491/19491 [==============================] - 31s - loss: 1.1472 - acc: 0.8607    \n",
      "Epoch 26/30\n",
      "19491/19491 [==============================] - 30s - loss: 1.1044 - acc: 0.8681    \n",
      "Epoch 27/30\n",
      "19491/19491 [==============================] - 32s - loss: 1.0593 - acc: 0.8736    \n",
      "Epoch 28/30\n",
      "19491/19491 [==============================] - 30s - loss: 1.0217 - acc: 0.8804    \n",
      "Epoch 29/30\n",
      "19491/19491 [==============================] - 31s - loss: 0.9883 - acc: 0.8843    \n",
      "Epoch 30/30\n",
      "19491/19491 [==============================] - 30s - loss: 0.9562 - acc: 0.8897    \n"
     ]
    }
   ],
   "source": [
    "model3.fit(input_, y=output_, batch_size=120, epochs=30, verbose=1)\n",
    "model3.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_F3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19491/19491 [==============================] - 12s    \n",
      "Accuracy: 90.282694577 %\n"
     ]
    }
   ],
   "source": [
    "scores = model3.evaluate(input_, output_, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_keras(inp):\n",
    "    idxs = [word_indices[c] for c in inp] #convert characters to indices\n",
    "    arrs = np.array(idxs)[np.newaxis,:] #converting to the required input format\n",
    "    p = model3.predict(arrs)[0] #using the model to predict the next index\n",
    "    return words[np.argmax(p)] #converting the index with max probability to a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "עוגה רצינית ב אספיק בבית אוכלת בנו\n",
      "אוקיי אז אין בחמישי בבירור דבר גזר\n",
      "ערב טוב בולבולים אתן בשיעור אדמה בשקם\n",
      "שחייבים להגיע בכללי דק הוא אין אותך\n",
      "מפחיד לתלות איתי בלילה גם בשישי את\n"
     ]
    }
   ],
   "source": [
    "#generate new sentences\n",
    "to_file=[]\n",
    "for i in range(0,5):\n",
    "    #select seed index- first word of the sentance\n",
    "    start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "    sen_to_file=''\n",
    "    w= words1[start_index:start_index+2]\n",
    "\n",
    "    while(w[0] == 'סוףהודעה' or w[1] == 'סוףהודעה'):\n",
    "        start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "        w= words1[start_index:start_index+2]\n",
    "   \n",
    "    w= words1[start_index:start_index+2]\n",
    "    sen_to_file+=w[0]+' '+w[1]\n",
    "    for i in range(0,5):\n",
    "        #predict the next words\n",
    "        next_word= get_next_keras(w)\n",
    "        if(next_word == 'סוףהודעה'):\n",
    "            break\n",
    "        sen_to_file+=' '+next_word\n",
    "        w= [w[1],next_word]\n",
    "    if(len(sen_to_file.split(' '))>2):\n",
    "            to_file.append(sen_to_file)\n",
    "            print(sen_to_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save to file\n",
    "with open('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\ff2_new.txt', 'w', newline='\\n', encoding='utf-8') as txt_file:\n",
    "    writer = csv.writer(txt_file, delimiter='\\n')\n",
    "    writer.writerow(to_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
