{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data  \n",
    "Mark end of message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_table('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\m.txt', header=None ,error_bad_lines=False)\n",
    "df1=df1.apply(lambda x: x+' סוףהודעה')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concat all messages\n",
    "X2=df1[0].tolist()\n",
    "text= ' '.join(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a sentence into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 22842\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "words1=text_to_word_sequence(text, lower=False, split=\" \")\n",
    "words = sorted(text_to_word_sequence(text, lower=False, split=\" \"))\n",
    "words.append(' ')\n",
    "vocab_size = len(words)\n",
    "print('total words:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetUniqueWords(words):\n",
    "    words_set = set()\n",
    "    for word in words:                \n",
    "        words_set.add(word)\n",
    "    return words_set\n",
    "\n",
    "unique_words = GetUniqueWords(words)\n",
    "number_of_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build index_to_word and word_to_index vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build index_to_word and word_to_index vectors\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "idx = [word_indices[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sen=df1[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut the text in semi-redundant sequences of maxlen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number sequences: 7612\n"
     ]
    }
   ],
   "source": [
    "maxlen = 5\n",
    "step = 3\n",
    "sentences = []\n",
    "next_words = []\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen words\n",
    "for i in range(0, len(words1) - maxlen, step):\n",
    "    sentences.append(words1[i: i + maxlen])\n",
    "    next_words.append(words1[i + maxlen])\n",
    "print('number sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp=pd.DataFrame(sentences)\n",
    "sentences=tmp[0]+' '+tmp[1]+' '+tmp[2]+' '+tmp[3]+' '+tmp[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(words)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(words)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(text_to_word_sequence(sentence, lower=False, split=\" \")):\n",
    "        X[i, t, word_indices[word]] = 1\n",
    "    y[i, word_indices[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_indices) + 1, 300))\n",
    "for word, i in word_indices.items():        \n",
    "    embedding_vector = sentences.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n",
    "Is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem\n",
    "\n",
    "* LSTM layer\n",
    "* A Dense layer with len(words) nodes\n",
    "* Activation function= softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               11761152  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22842)             2946618   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 22842)             0         \n",
      "=================================================================\n",
      "Total params: 14,707,770\n",
      "Trainable params: 14,707,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(words))))\n",
    "#a Dense layer with len(words) nodes\n",
    "model.add(Dense(len(words)))\n",
    "#Activation function= softmax\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model  \n",
    "We use the RMSProp optimizer  \n",
    "We use the sparse_categorical_crossentropy loss that accepts sparse labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test, train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from array import array\n",
    "import random\n",
    "from random import randrange\n",
    "test_x=[]\n",
    "test_y=[]\n",
    "\n",
    "index=[]\n",
    "for i in range(0,900):\n",
    "    random_index = randrange(0,len(X))\n",
    "    index.append(random_index)\n",
    "    test_x.append(X[random_index])\n",
    "    test_y.append(y[random_index])\n",
    "    \n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "train_x = np.delete(X, index, axis=0)\n",
    "train_y = np.delete(y, index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train the model, fit it to the train data\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=10,validation_split=0.2)\n",
    "model.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 7s     \n",
      "Accuracy: 64.6666666667 %\n"
     ]
    }
   ],
   "source": [
    "scores = model1.evaluate(test_x, test_y, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model  \n",
    "Fit it to the all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model, fit it to the data\n",
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,validation_split=0.2)\n",
    "\n",
    "#save the model\n",
    "model.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7612/7612 [==============================] - 71s    \n",
      "Accuracy: 99.8423541776 %\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, y, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def concatenate_list_data(list):\n",
    "    result= ''\n",
    "    for element in list:\n",
    "        result += str(element)+\" \"\n",
    "    return result[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate new sentences\n",
    "to_file=[]\n",
    "for i in range(0,1500):\n",
    "    #select seed index- first word of the sentance\n",
    "    start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "    while(words1[start_index:start_index+1][0] == 'סוףהודעה'):\n",
    "        start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "\n",
    "    for diversity in [1.0]:\n",
    "        generated = ''\n",
    "        sentence = words1[start_index: start_index+1]\n",
    "        generated += concatenate_list_data(sentence)\n",
    "\n",
    "        sys.stdout.write(generated)\n",
    "        sen_to_file=generated\n",
    "\n",
    "        next_word=''\n",
    "        z=0\n",
    "        for i in range(0,10):\n",
    "            x = np.zeros((1, maxlen, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t, word_indices[word]] = 1.\n",
    "            #predict the next words\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            generated += next_word+' '\n",
    "            sentence = sentence[1:] + list([next_word])\n",
    "            if(next_word == 'סוףהודעה'):\n",
    "                break\n",
    "            sen_to_file+=' '+next_word\n",
    "            z+=1\n",
    "            sys.stdout.write(' '+next_word)              \n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        if(len(sen_to_file.split(' '))>1):\n",
    "            to_file.append(sen_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# #save to file\n",
    "# with open('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\mm_new.txt', 'w', newline='\\n', encoding='utf-8') as txt_file:\n",
    "#     writer = csv.writer(txt_file, delimiter='\\n')\n",
    "#     writer.writerow(to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#read the data\n",
    "df1 = pd.read_table('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\m.txt', header=None ,error_bad_lines=False)\n",
    "#mark end of message\n",
    "df1=df1.apply(lambda x: x+' סוףהודעה')\n",
    "\n",
    "# import re\n",
    "regex = re.compile('[^םןאבגדהוזחטיכלמנסעפצקרשתץףך?! ].*')\n",
    "df1[0]=df1[0].apply(lambda x: regex.sub('', x))\n",
    "df1=df1.dropna()\n",
    "df1=df1[df1[0]!=' ']\n",
    "\n",
    "#concat all messages\n",
    "text= ' '.join(df1[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 20104\n",
      "unique_words: 4569\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "words1=text_to_word_sequence(text, lower=False, split=\" \")\n",
    "words = sorted(text_to_word_sequence(text, lower=False, split=\" \"))\n",
    "words.append(' ')\n",
    "vocab_size = len(words)\n",
    "print('total words:', vocab_size)\n",
    "\n",
    "unique_words = GetUniqueWords(words)\n",
    "number_of_words = len(unique_words)\n",
    "print('unique_words:', number_of_words)\n",
    "\n",
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(unique_words))\n",
    "idx = [word_indices[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               11761152  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22842)             2946618   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 22842)             0         \n",
      "=================================================================\n",
      "Total params: 14,707,770\n",
      "Trainable params: 14,707,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "cs = 2\n",
    "\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, 1)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, 1)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, 1)] \n",
    "\n",
    "x1 = np.array(c1_dat)\n",
    "x2 = np.array(c2_dat)\n",
    "x3 = np.array(c3_dat)\n",
    "    \n",
    "input_ = np.stack([x1,x2],axis=1)\n",
    "output_ = np.stack([x3],axis=1)\n",
    "\n",
    "n_fac = 42\n",
    "n_hidden = 256\n",
    "\n",
    "#build the model \n",
    "model3=Sequential([\n",
    "    Embedding(number_of_words, n_fac, input_length=cs),\n",
    "    LSTM(n_hidden, return_sequences=False, activation='relu'),        \n",
    "    Dense(number_of_words, activation='softmax'),\n",
    "])    \n",
    "  \n",
    "print(model.summary()) \n",
    "#compile\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop' ,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from array import array\n",
    "import random\n",
    "from random import randrange\n",
    "#split to train and test set\n",
    "\n",
    "test_x=[]\n",
    "test_y=[]\n",
    "\n",
    "index=[]\n",
    "for i in range(0,900):\n",
    "    random_index = randrange(0,len(input_))\n",
    "    index.append(random_index)\n",
    "    test_x.append(input_[random_index])\n",
    "    test_y.append(output_[random_index])\n",
    "    \n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "train_x = np.delete(input_, index, axis=0)\n",
    "train_y = np.delete(output_, index, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "19220/19220 [==============================] - 28s - loss: 5.7184 - acc: 0.2227    \n",
      "Epoch 2/30\n",
      "19220/19220 [==============================] - 27s - loss: 4.6631 - acc: 0.3294    \n",
      "Epoch 3/30\n",
      "19220/19220 [==============================] - 27s - loss: 4.0843 - acc: 0.4439    \n",
      "Epoch 4/30\n",
      "19220/19220 [==============================] - 26s - loss: 3.6930 - acc: 0.5142    \n",
      "Epoch 5/30\n",
      "19220/19220 [==============================] - 27s - loss: 3.4593 - acc: 0.5524    \n",
      "Epoch 6/30\n",
      "19220/19220 [==============================] - 28s - loss: 3.2981 - acc: 0.5781    \n",
      "Epoch 7/30\n",
      "19220/19220 [==============================] - 28s - loss: 3.1642 - acc: 0.5999    \n",
      "Epoch 8/30\n",
      "19220/19220 [==============================] - 27s - loss: 3.0599 - acc: 0.6181    \n",
      "Epoch 9/30\n",
      "19220/19220 [==============================] - 27s - loss: 2.9770 - acc: 0.6292    \n",
      "Epoch 10/30\n",
      "19220/19220 [==============================] - 29s - loss: 2.9013 - acc: 0.6434    \n",
      "Epoch 11/30\n",
      "19220/19220 [==============================] - 27s - loss: 2.8318 - acc: 0.6549    \n",
      "Epoch 12/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.7701 - acc: 0.6642    \n",
      "Epoch 13/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.7085 - acc: 0.6742    \n",
      "Epoch 14/30\n",
      "19220/19220 [==============================] - 29s - loss: 2.6484 - acc: 0.6815    \n",
      "Epoch 15/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.5910 - acc: 0.6890    \n",
      "Epoch 16/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.5331 - acc: 0.6947    \n",
      "Epoch 17/30\n",
      "19220/19220 [==============================] - 27s - loss: 2.4780 - acc: 0.7012    \n",
      "Epoch 18/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.4227 - acc: 0.7072    \n",
      "Epoch 19/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.3713 - acc: 0.7129    \n",
      "Epoch 20/30\n",
      "19220/19220 [==============================] - 29s - loss: 2.3211 - acc: 0.7180    \n",
      "Epoch 21/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.2704 - acc: 0.7230    \n",
      "Epoch 22/30\n",
      "19220/19220 [==============================] - 28s - loss: 2.2188 - acc: 0.7297    \n",
      "Epoch 23/30\n",
      "19220/19220 [==============================] - 29s - loss: 2.1703 - acc: 0.7332    \n",
      "Epoch 24/30\n",
      "19220/19220 [==============================] - 30s - loss: 2.1219 - acc: 0.7405    \n",
      "Epoch 25/30\n",
      "19220/19220 [==============================] - 29s - loss: 2.0735 - acc: 0.7466    \n",
      "Epoch 26/30\n",
      "19220/19220 [==============================] - 29s - loss: 2.0219 - acc: 0.7522    \n",
      "Epoch 27/30\n",
      "19220/19220 [==============================] - 29s - loss: 1.9649 - acc: 0.7589    \n",
      "Epoch 28/30\n",
      "19220/19220 [==============================] - 30s - loss: 1.9053 - acc: 0.7659    \n",
      "Epoch 29/30\n",
      "19220/19220 [==============================] - 28s - loss: 1.8433 - acc: 0.7737    \n",
      "Epoch 30/30\n",
      "19220/19220 [==============================] - 27s - loss: 1.7900 - acc: 0.7805    \n"
     ]
    }
   ],
   "source": [
    "model3.fit(train_x, y=train_y, batch_size=120, epochs=30, verbose=1)\n",
    "model3.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M3_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M3_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/900 [============================>.] - ETA: 0sAccuracy: 71.0 %\n"
     ]
    }
   ],
   "source": [
    "scores = model4.evaluate(test_x, test_y, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model  \n",
    "Fit it to the all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20101/20101 [==============================] - 29s - loss: 1.8195 - acc: 0.7834    \n",
      "Epoch 2/30\n",
      "20101/20101 [==============================] - 30s - loss: 1.7766 - acc: 0.7899    \n",
      "Epoch 3/30\n",
      "20101/20101 [==============================] - 31s - loss: 1.7246 - acc: 0.7938    \n",
      "Epoch 4/30\n",
      "20101/20101 [==============================] - 30s - loss: 1.6678 - acc: 0.7992    \n",
      "Epoch 5/30\n",
      "20101/20101 [==============================] - 30s - loss: 1.6085 - acc: 0.8036    \n",
      "Epoch 6/30\n",
      "20101/20101 [==============================] - 29s - loss: 1.5432 - acc: 0.8050    \n",
      "Epoch 7/30\n",
      "20101/20101 [==============================] - 28s - loss: 1.4714 - acc: 0.8075    \n",
      "Epoch 8/30\n",
      "20101/20101 [==============================] - 31s - loss: 1.3893 - acc: 0.8144    \n",
      "Epoch 9/30\n",
      "20101/20101 [==============================] - 30s - loss: 1.3372 - acc: 0.8232    \n",
      "Epoch 10/30\n",
      "20101/20101 [==============================] - 31s - loss: 1.3090 - acc: 0.8290    \n",
      "Epoch 11/30\n",
      "20101/20101 [==============================] - 29s - loss: 1.2796 - acc: 0.8336    \n",
      "Epoch 12/30\n",
      "20101/20101 [==============================] - 35s - loss: 1.2653 - acc: 0.8367    \n",
      "Epoch 13/30\n",
      "20101/20101 [==============================] - 33s - loss: 1.2430 - acc: 0.8385    \n",
      "Epoch 14/30\n",
      "20101/20101 [==============================] - 30s - loss: 1.2229 - acc: 0.8427    \n",
      "Epoch 15/30\n",
      "20101/20101 [==============================] - 31s - loss: 1.2086 - acc: 0.8435    \n",
      "Epoch 16/30\n",
      "20101/20101 [==============================] - 28s - loss: 1.1906 - acc: 0.8462    \n",
      "Epoch 17/30\n",
      "20101/20101 [==============================] - 28s - loss: 1.1752 - acc: 0.8479    \n",
      "Epoch 18/30\n",
      "20101/20101 [==============================] - 27s - loss: 1.1653 - acc: 0.8504    \n",
      "Epoch 19/30\n",
      "20101/20101 [==============================] - 27s - loss: 1.1427 - acc: 0.8529    \n",
      "Epoch 20/30\n",
      "20101/20101 [==============================] - 28s - loss: 1.1258 - acc: 0.8539    \n",
      "Epoch 21/30\n",
      "20101/20101 [==============================] - 28s - loss: 1.1033 - acc: 0.8563    \n",
      "Epoch 22/30\n",
      "20101/20101 [==============================] - 26s - loss: 1.0634 - acc: 0.8568    \n",
      "Epoch 23/30\n",
      "20101/20101 [==============================] - 27s - loss: 1.0193 - acc: 0.8599    \n",
      "Epoch 24/30\n",
      "20101/20101 [==============================] - 30s - loss: 0.9809 - acc: 0.8627    \n",
      "Epoch 25/30\n",
      "20101/20101 [==============================] - 31s - loss: 0.9527 - acc: 0.8656    \n",
      "Epoch 26/30\n",
      "20101/20101 [==============================] - 31s - loss: 0.9272 - acc: 0.8691    \n",
      "Epoch 27/30\n",
      "20101/20101 [==============================] - 30s - loss: 0.9121 - acc: 0.8707    \n",
      "Epoch 28/30\n",
      "20101/20101 [==============================] - 29s - loss: 0.8914 - acc: 0.8734    \n",
      "Epoch 29/30\n",
      "20101/20101 [==============================] - 32s - loss: 0.8671 - acc: 0.8772    \n",
      "Epoch 30/30\n",
      "20101/20101 [==============================] - 33s - loss: 0.8496 - acc: 0.8778    \n"
     ]
    }
   ],
   "source": [
    "model3.fit(input_, y=output_, batch_size=120, epochs=30, verbose=1)\n",
    "model3.save('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = load_model('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\RNN_M3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20096/20101 [============================>.] - ETA: 0sAccuracy: 89.2841152187 %\n"
     ]
    }
   ],
   "source": [
    "scores = model3.evaluate(input_, output_, verbose=1)\n",
    "print(\"Accuracy:\",scores[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_keras(inp):\n",
    "    idxs = [word_indices[c] for c in inp] #convert characters to indices\n",
    "    arrs = np.array(idxs)[np.newaxis,:] #converting to the required input format\n",
    "    p = model3.predict(arrs)[0] #using the model to predict the next index\n",
    "    return words[np.argmax(p)] #converting the index with max probability to a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate new sentences\n",
    "to_file=[]\n",
    "for i in range(0,1500):\n",
    "    #select seed index- first word of the sentance\n",
    "    start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "    sen_to_file=''\n",
    "    w= words1[start_index:start_index+2]\n",
    "\n",
    "    while(w[0] == 'סוףהודעה' or w[1] == 'סוףהודעה'):\n",
    "        start_index = random.randint(0, len(words1) - maxlen - 1)\n",
    "        w= words1[start_index:start_index+2]\n",
    "   \n",
    "    w= words1[start_index:start_index+2]\n",
    "    sen_to_file+=w[0]+' '+w[1]\n",
    "    for i in range(0,5):\n",
    "        #predict the next words\n",
    "        next_word= get_next_keras(w)\n",
    "        if(next_word == 'סוףהודעה'):\n",
    "            break\n",
    "        sen_to_file+=' '+next_word\n",
    "        w= [w[1],next_word]\n",
    "    if(len(sen_to_file.split(' '))>2):\n",
    "            to_file.append(sen_to_file)\n",
    "            print(sen_to_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#save to file\n",
    "with open('C:\\\\Users\\\\DELL\\\\Desktop\\\\data_scientist\\\\rnn\\\\mm2_new.txt', 'w', newline='\\n', encoding='utf-8') as txt_file:\n",
    "    writer = csv.writer(txt_file, delimiter='\\n')\n",
    "    writer.writerow(to_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
